Python spark application

dependencies:
- python
- pyspark (pip install pyspark)

- mongo-spark ($SPARK_HOME/bin/spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.12:2.4.0)



# Run:
# ./bin/pyspark --conf "spark.mongodb.input.uri=mongodb://mongo-0.mongodb-service/test.myCollection?readPreference=primaryPreferred" \
#               --conf "spark.mongodb.output.uri=mongodb://mongo-0.mongodb-service/test.myCollection" \
#               --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0


sudo apt install python-pip
pip install sparkpi

[...]/bin/spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.12:2.4.0

KUBERNETES_IP=$(kubectl cluster-info | grep master | grep -o 'http.*:[0-9]*')


spark-2.4.0-bin-hadoop2.7/bin/spark-submit \
    --master k8s://${KUBERNETES_IP} \
    --deploy-mode cluster \
    --name spark-test \
    --conf spark.executor.instances=2 \
    --driver-memory=1g \
    --executor-memory=1g \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
    --conf spark.kubernetes.container.image=docker.io/theipple/spark \
    app.py



spark-2.4.0-bin-hadoop2.7/bin/spark-submit \
    --master k8s://${KUBERNETES_IP} \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=2 \
    --driver-memory=1g \
    --executor-memory=1g \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
    --conf spark.kubernetes.container.image=docker.io/theipple/spark \
    local:///opt/spark/examples/jars/spark-examples_2.11-2.4.0.jar






#
